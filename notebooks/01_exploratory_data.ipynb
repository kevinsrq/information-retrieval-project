{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise exploratória de dados\n",
    "\n",
    "A análise exploratória de dados é uma etapa fundamental para qualquer projeto que envolva dados. Ela consiste em examinar e estudar as características de um conjunto de dados, como sua distribuição, suas variáveis, suas relações e suas anomalias, antes de aplicar técnicas mais avançadas de estatística ou aprendizagem de máquina. O objetivo da análise exploratória de dados é obter uma compreensão mais profunda dos dados, identificar padrões, tendências e insights que possam orientar a tomada de decisão e a solução de problemas. A análise exploratória de dados pode ser realizada de forma visual, usando gráficos, tabelas e diagramas, ou de forma numérica, usando medidas de tendência central, dispersão, correlação e teste de hipóteses. A escolha dos métodos depende do tipo e da qualidade dos dados disponíveis, bem como da pergunta ou objetivo que se quer responder com a anál\n",
    "\n",
    "<!-- <hr style=\"border-width: 1px\" width=\"95%\" > -->\n",
    "<div></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa os módulos necessários\n",
    "import numpy as np   # Módulo para trabalhar com matrizes e funções matemáticas\n",
    "import pandas as pd  # Módulo para trabalhar com dataframes e séries em Python\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div></div> \n",
    "\n",
    "## Estruturação dos Arquivos\n",
    "\n",
    "<div></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the directory path to where the data is stored\n",
    "doc_dir = '../data/emails/mini_newsgroups/'\n",
    "\n",
    "# Create an empty list to hold the processed data\n",
    "database = []\n",
    "\n",
    "# Iterate over each file in the directory and its subdirectories\n",
    "for filepath in os.listdir(doc_dir): \n",
    "    for filename in os.listdir(f'{doc_dir}{filepath}'):\n",
    "\n",
    "        # Open each file individually and read its contents\n",
    "        with open(os.path.join(doc_dir, filepath, filename), 'r') as f:\n",
    "            text_data = f.read().strip()\n",
    "        \n",
    "        # Split the header and body of the email\n",
    "        try:\n",
    "            header, body = text_data.split('\\n\\n', maxsplit=1)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # Convert header to a dictionary\n",
    "        header_dict = {}\n",
    "        for line in header.split('\\n'):\n",
    "            try:\n",
    "                # Split the key and value in each header field and store them in a dictionary\n",
    "                key, value = line.strip().split(': ', maxsplit=1)\n",
    "                header_dict[key] = value\n",
    "            except:\n",
    "                # If a header field cannot be split properly, skip it and continue\n",
    "                continue\n",
    "        \n",
    "        # Append the processed data to the list\n",
    "        database.append({'filepath': filepath, \n",
    "                        'filename': filename,\n",
    "                        'text': body, \n",
    "                        **header_dict\n",
    "                        })\n",
    "\n",
    "# tranformation from dict -> dataframe\n",
    "base_inicial = pd.DataFrame(database)\n",
    "\n",
    "# remove database from memory\n",
    "del database\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div></div> \n",
    "\n",
    "## Processamento de Texto\n",
    "\n",
    "<div></div> \n",
    "\n",
    "### Transformação de minúsculos\n",
    "\n",
    "<div></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substituindo todos os caracteres que não são letras ou números por espaços em branco, exceto as barras invertidas (\\\\)\n",
    "base_inicial['text'] = base_inicial['text'].replace(r'(\\\\[a-z])|([^\\w\\\\])', ' ', regex=True)\n",
    "\n",
    "# Aplicando as funções str.lower() e str.strip() simultaneamente\n",
    "base_inicial['text'] = base_inicial['text'].apply(lambda x: x.lower().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       article n4hy 93apr5120934 harder ccr p ida org...\n",
       "1       kmr4 po cwru edu keith ryan writes people keep...\n",
       "2       livesey solntze wpd sgi com jon livesey writes...\n",
       "3       bobbe vice ico tek com robert beauchaine write...\n",
       "4       article 16ba1e927 drporter suvm syr edu drport...\n",
       "                              ...                        \n",
       "1994    article 1rgmjn 567 access digex net huston acc...\n",
       "1995    started reading newsgroup following thread cur...\n",
       "1996    blessed hunger thirst righteousness filled mat...\n",
       "1997    curious know christians ever read books based ...\n",
       "1998    article 1993apr27 073723 18577 csis dit csiro ...\n",
       "Name: text, Length: 1999, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Faz o download do recurso 'stopwords' do nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define a lista de stopwords em inglês usando o módulo stopwords do nltk\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# Aplica a função lambda em cada linha da coluna 'text' da tabela 'base_inicial'\n",
    "# A função lambda realiza a tokenização do texto, transforma as palavras em minúsculas e remove as stopwords\n",
    "base_inicial['text'].apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stopwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')    # faz o download do recurso 'wordnet' do nltk\n",
    "nltk.download('punkt')     # faz o download do recurso 'punkt' do nltk\n",
    "\n",
    "# Cria um objeto 'w_tokenizer' da classe 'WhitespaceTokenizer' do nltk para tokenizar o texto por espaços em branco\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "\n",
    "# Cria um objeto 'lemmatizer' da classe 'WordNetLemmatizer' do nltk para realizar a lematização das palavras\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Define a função 'lemmatizer_text' que recebe um texto como entrada, tokeniza o texto em palavras e lematiza cada palavra\n",
    "def lemmatizer_text(text): \n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "# Cria uma nova coluna 'tokens' na tabela 'base_inicial' que contém uma lista de tokens lematizados para cada texto\n",
    "base_inicial['tokens'] = base_inicial['text'].map(lemmatizer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma lista de palavras a partir da lista de tokens\n",
    "w = [j for i in list(itertools.chain(base_inicial['tokens'])) for j in i]\n",
    "\n",
    "# Instancia um objeto SpellChecker para correção ortográfica\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Cria um dicionário com as palavras únicas da lista, faz a correção ortográfica e associa com a palavra original\n",
    "spell_checked = {word: spell.correction(word) for word in pd.Series(w).unique()}\n",
    "\n",
    "# Define o caminho do arquivo que irá armazenar o dicionário serializado\n",
    "path = '../references/spellcheck.pickle'\n",
    "\n",
    "# Abre o arquivo para gravação em modo binário e escreve o objeto serializado\n",
    "with open(path, 'wb') as file: \n",
    "    pickle.dump(spell_checked, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export da base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'n'.lower() in ['s', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "File already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m         base_inicial\u001b[39m.\u001b[39mto_parquet(path, compression\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileExistsError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mFile already exists\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[39melse\u001b[39;00m: \n\u001b[0;32m     10\u001b[0m     base_inicial\u001b[39m.\u001b[39mto_parquet(path, compression\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mFileExistsError\u001b[0m: File already exists"
     ]
    }
   ],
   "source": [
    "path = '../data/processed/base_processed.parquet.gzip'\n",
    "\n",
    "if os.path.isfile(path): \n",
    "    answer = input('File already exists, do you want to overwrite? (y/n)')\n",
    "    if answer.lower() in ['s', 'y']:\n",
    "        base_inicial.to_parquet(path, compression='gzip')\n",
    "    else:\n",
    "        raise FileExistsError('File already exists')\n",
    "else: \n",
    "    base_inicial.to_parquet(path, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
