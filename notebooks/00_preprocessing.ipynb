{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento dos Documentos\n",
    "\n",
    "A limpeza dos dados é um processo essencial para garantir a qualidade e a confiabilidade das informações armazenadas em um banco de dados. A limpeza dos dados envolve a identificação e a correção de erros, inconsistências, duplicidades e valores ausentes nos dados. A arquitetura do armazenamento é a forma como os dados são organizados, estruturados e acessados em um banco de dados. Uma das opções de arquitetura é o formato YAML, que significa YAML Ain't Markup Language. O YAML é um formato de serialização de dados que usa uma sintaxe simples e legível para representar estruturas de dados como listas, mapas, sequências e escalares. O YAML é compatível com diversas linguagens de programação e pode ser usado para armazenar dados de forma hierárquica e flexível.\n",
    "\n",
    "<!-- <hr style=\"border-width: 1px\" width=\"95%\" > -->\n",
    "<div></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa os módulos necessários\n",
    "import os    # Módulo para lidar com funções do sistema operacional\n",
    "import gc    # Módulo para realizar coleta de lixo e gerenciamento de memória\n",
    "import pickle # Módulo para serialização e desserialização de objetos em Python\n",
    "import itertools # Módulo para criação de iteráveis\n",
    "\n",
    "import numpy as np   # Módulo para trabalhar com matrizes e funções matemáticas\n",
    "import pandas as pd  # Módulo para trabalhar com dataframes e séries em Python\n",
    "\n",
    "import nltk # Módulo para processamento de linguagem natural\n",
    "from nltk.corpus import stopwords # Módulo para importar lista de stopwords em inglês\n",
    "from spellchecker import SpellChecker # Módulo para correção ortográfica de palavras\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div></div> \n",
    "\n",
    "## Estruturação dos Arquivos\n",
    "\n",
    "<div></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the directory path to where the data is stored\n",
    "doc_dir = '../data/emails/20_newsgroups/'\n",
    "\n",
    "# Create an empty list to hold the processed data\n",
    "database = []\n",
    "\n",
    "# Iterate over each file in the directory and its subdirectories\n",
    "def process_files(doc_dir): \n",
    "    \n",
    "    database = [] \n",
    "    \n",
    "    for filepath in os.listdir(doc_dir): \n",
    "        \n",
    "        for filename in os.listdir(f'{doc_dir}{filepath}'):\n",
    "\n",
    "            # Open each file individually and read its contents\n",
    "            with open(os.path.join(doc_dir, filepath, filename), 'r') as f:\n",
    "                text_data = f.read().strip()\n",
    "\n",
    "            # Split the header and body of the email\n",
    "            try:\n",
    "                header, body = text_data.split('\\n\\n', maxsplit=1)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # Convert header to a dictionary\n",
    "            header_dict = {}\n",
    "            for line in header.split('\\n'):\n",
    "                try:\n",
    "                    # Split the key and value in each header field and store them in a dictionary\n",
    "                    key, value = line.strip().split(': ', maxsplit=1)\n",
    "                    header_dict[key] = value\n",
    "                except:\n",
    "                    # If a header field cannot be split properly, skip it and continue\n",
    "                    continue\n",
    "\n",
    "            # Append the processed data to the list\n",
    "            database.append({'filepath': filepath, \n",
    "                            'filename': filename,\n",
    "                            'body': body, \n",
    "                            **header_dict,\n",
    "                            'text': text_data\n",
    "                            })\n",
    "    return database\n",
    "\n",
    "database = process_files(doc_dir)\n",
    "\n",
    "# tranformation from dict -> dataframe\n",
    "base_inicial = pd.DataFrame(database)\n",
    "\n",
    "# remove database from memory\n",
    "del database\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div></div> \n",
    "\n",
    "## Processamento de Texto\n",
    "\n",
    "<div></div> \n",
    "\n",
    "### Transformação de minúsculos\n",
    "\n",
    "<div></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substituindo todos os caracteres que não são letras ou números por espaços em branco, exceto as barras invertidas (\\\\)\n",
    "base_inicial['text'] = base_inicial['text'].replace(r'(\\\\[a-z])|([^\\w\\\\])|(\\S+\\d\\S+)', ' ', regex=True)\n",
    "\n",
    "# Aplicando as funções str.lower() e str.strip() simultaneamente\n",
    "base_inicial['text'] = base_inicial['text'].apply(lambda x: x.lower().strip())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div></div> \n",
    "\n",
    "### Tokenização e Lemmatizer\n",
    "\n",
    "<div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')    # faz o download do recurso 'wordnet' do nltk\n",
    "nltk.download('punkt')     # faz o download do recurso 'punkt' do nltk\n",
    "\n",
    "# Cria um objeto 'w_tokenizer' da classe 'WhitespaceTokenizer' do nltk para tokenizar o texto por espaços em branco\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "\n",
    "# Cria um objeto 'lemmatizer' da classe 'WordNetLemmatizer' do nltk para realizar a lematização das palavras\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Define a função 'lemmatizer_text' que recebe um texto como entrada, tokeniza o texto em palavras e lematiza cada palavra\n",
    "def lemmatizer_text(text): \n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "# Cria uma nova coluna 'tokens' na tabela 'base_inicial' que contém uma lista de tokens lematizados para cada texto\n",
    "base_inicial['tokens'] = base_inicial['text'].map(lemmatizer_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div></div> \n",
    "\n",
    "### Remoção de Stopwords\n",
    "\n",
    "<div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Faz o download do recurso 'stopwords' do nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define a lista de stopwords em inglês usando o módulo stopwords do nltk\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# Aplica a função lambda em cada linha da coluna 'text' da tabela 'base_inicial'\n",
    "# A função lambda realiza a tokenização do texto, transforma as palavras em minúsculas e remove as stopwords\n",
    "base_inicial['text'] = base_inicial['text'].apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stopwords))\n",
    "base_inicial['tokens'] = base_inicial['tokens'].apply(lambda words: [word.lower() for word in words if word not in stopwords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cria uma lista de palavras a partir da lista de tokens\n",
    "# w = [j for i in list(itertools.chain(base_inicial['tokens'])) for j in i]\n",
    "\n",
    "# # Instancia um objeto SpellChecker para correção ortográfica\n",
    "# spell = SpellChecker()\n",
    "\n",
    "# if !os.path.isfile(): \n",
    "#     # Cria um dicionário com as palavras únicas da lista, faz a correção ortográfica e associa com a palavra original\n",
    "#     spell_checked = {word: spell.correction(word) for word in pd.Series(w).unique()}\n",
    "\n",
    "#     # Define o caminho do arquivo que irá armazenar o dicionário serializado\n",
    "#     path = '../references/spellcheck.pickle'\n",
    "\n",
    "#     # Abre o arquivo para gravação em modo binário e escreve o objeto serializado\n",
    "#     with open(path, 'wb') as file: \n",
    "#         pickle.dump(spell_checked, file)\n",
    "# else: \n",
    "#     pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export da base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/processed/base_processed.parquet.gzip'\n",
    "\n",
    "if os.path.isfile(path): \n",
    "    answer = input('File already exists, do you want to overwrite? (y/n)')\n",
    "    if answer.lower() in ['s', 'y']:\n",
    "        base_inicial.to_parquet(path, compression='gzip')\n",
    "    else:\n",
    "        raise FileExistsError('File already exists')\n",
    "else: \n",
    "    base_inicial.to_parquet(path, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
