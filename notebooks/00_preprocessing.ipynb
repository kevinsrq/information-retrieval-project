{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento dos Documentos\n",
    "\n",
    "A limpeza dos dados é um processo essencial para garantir a qualidade e a confiabilidade das informações armazenadas em um banco de dados. A limpeza dos dados envolve a identificação e a correção de erros, inconsistências, duplicidades e valores ausentes nos dados. A arquitetura do armazenamento é a forma como os dados são organizados, estruturados e acessados em um banco de dados. Uma das opções de arquitetura é o formato YAML, que significa YAML Ain't Markup Language. O YAML é um formato de serialização de dados que usa uma sintaxe simples e legível para representar estruturas de dados como listas, mapas, sequências e escalares. O YAML é compatível com diversas linguagens de programação e pode ser usado para armazenar dados de forma hierárquica e flexível.\n",
    "\n",
    "<!-- <hr style=\"border-width: 1px\" width=\"95%\" > -->\n",
    "<div></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa os módulos necessários\n",
    "import os    # Módulo para lidar com funções do sistema operacional\n",
    "import gc    # Módulo para realizar coleta de lixo e gerenciamento de memória\n",
    "\n",
    "import numpy as np   # Módulo para trabalhar com matrizes e funções matemáticas\n",
    "import pandas as pd  # Módulo para trabalhar com dataframes e séries em Python\n",
    "\n",
    "import nltk # Módulo para processamento de linguagem natural\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div></div> \n",
    "\n",
    "## Estruturação dos Arquivos\n",
    "\n",
    "<div></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caminho das queries \n",
    "query_path = '../data/emails/mini_newsgroups/misc.forsale/'\n",
    "\n",
    "# caminho dos documentos\n",
    "docs_path = '../data/emails/20_newsgroups/misc.forsale/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(doc_dir):\n",
    "    # Use a list comprehension to get a list of file paths\n",
    "    database = [{'filepath': doc_dir,\n",
    "                 'filename': filename,\n",
    "                 'text': open(os.path.join(doc_dir, filename), 'r').read().strip()}\n",
    "                 for filename in os.listdir(doc_dir)]\n",
    "\n",
    "    return database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import das bases\n",
    "database_docs = read_files(query_path)\n",
    "database_query = read_files(query_path)\n",
    "\n",
    "base_docs = pd.DataFrame(database_docs)\n",
    "base_query = pd.DataFrame(database_query)\n",
    "\n",
    "# Marcação das bases\n",
    "base_docs['tag'] = 'doc'\n",
    "base_query['tag'] = 'query'\n",
    "\n",
    "# junção das bases \n",
    "base = pd.concat([base_docs, base_query])\n",
    "\n",
    "del base_docs, base_query, database_docs, database_query\n",
    "gc.collect()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div></div> \n",
    "\n",
    "## Processamento de Texto\n",
    "\n",
    "<div></div> \n",
    "\n",
    "### Transformação de minúsculos\n",
    "\n",
    "<div></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\\[a-z]): para encontrar todos os caracteres que começam com uma barra invertida () seguida por uma letra minúscula (a-z);\n",
    "# ([^\\w\\]): para encontrar todos os caracteres que não são letras, números ou barras invertidas ();\n",
    "# (\\S+\\d\\S+): para encontrar todos os trechos de texto que contêm um ou mais caracteres não brancos (\\S), seguidos por um dígito (\\d), seguidos por mais um ou mais caracteres não brancos (\\S).\n",
    "base['post'] = base['text'].replace(r'(\\\\[a-z])|([^\\w\\\\])|(\\S+\\d\\S+)', ' ', regex=True)\n",
    "\n",
    "\n",
    "# Aplicando as funções str.lower() e str.strip() simultaneamente\n",
    "base['post'] = base['post'].apply(lambda x: x.lower().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ir.preprocessing import PreProcessing as pp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'PreProcessing' has no attribute 'tfidf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pp\u001b[39m.\u001b[39;49mtfidf(base, \u001b[39m'\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'PreProcessing' has no attribute 'tfidf'"
     ]
    }
   ],
   "source": [
    "pp.tfidf(base, 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edu</th>\n",
       "      <td>0.769037</td>\n",
       "      <td>0.769037</td>\n",
       "      <td>0.553274</td>\n",
       "      <td>0.612645</td>\n",
       "      <td>0.702170</td>\n",
       "      <td>0.702170</td>\n",
       "      <td>0.476731</td>\n",
       "      <td>0.476731</td>\n",
       "      <td>0.702170</td>\n",
       "      <td>0.368849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661155</td>\n",
       "      <td>0.476731</td>\n",
       "      <td>0.702170</td>\n",
       "      <td>0.368849</td>\n",
       "      <td>0.368849</td>\n",
       "      <td>0.553274</td>\n",
       "      <td>0.661155</td>\n",
       "      <td>0.702170</td>\n",
       "      <td>0.661155</td>\n",
       "      <td>0.184425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wpi</th>\n",
       "      <td>23.817975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.392922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.578730</td>\n",
       "      <td>0.152003</td>\n",
       "      <td>0.456009</td>\n",
       "      <td>0.152003</td>\n",
       "      <td>0.152003</td>\n",
       "      <td>0.304006</td>\n",
       "      <td>0.392922</td>\n",
       "      <td>0.152003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.304006</td>\n",
       "      <td>0.392922</td>\n",
       "      <td>0.544925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152003</td>\n",
       "      <td>0.152003</td>\n",
       "      <td>0.152003</td>\n",
       "      <td>0.504943</td>\n",
       "      <td>0.152003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs</th>\n",
       "      <td>1.023461</td>\n",
       "      <td>0.395929</td>\n",
       "      <td>0.395929</td>\n",
       "      <td>0.395929</td>\n",
       "      <td>0.791857</td>\n",
       "      <td>0.395929</td>\n",
       "      <td>0.395929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.791857</td>\n",
       "      <td>0.395929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.791857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.791857</td>\n",
       "      <td>0.395929</td>\n",
       "      <td>0.791857</td>\n",
       "      <td>0.395929</td>\n",
       "      <td>0.395929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>msrp</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.643856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scotts</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.643856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbking</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.643856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sherman</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.643856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mpd</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.643856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3654 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5    \n",
       "          0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \\\n",
       "edu       0.769037  0.769037  0.553274  0.612645  0.702170  0.702170   \n",
       "wpi      23.817975  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "to        0.392922  0.000000  0.578730  0.152003  0.456009  0.152003   \n",
       "cs        1.023461  0.395929  0.395929  0.395929  0.791857  0.395929   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "msrp      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "scotts    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "bbking    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "sherman   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "mpd       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "               6         7         8         9   ...        90        91   \n",
       "         0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  \\\n",
       "edu      0.476731  0.476731  0.702170  0.368849  ...  0.661155  0.476731   \n",
       "wpi      0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "to       0.152003  0.304006  0.392922  0.152003  ...  0.304006  0.392922   \n",
       "cs       0.395929  0.000000  0.791857  0.395929  ...  0.395929  0.000000   \n",
       "...           ...       ...       ...       ...  ...       ...       ...   \n",
       "msrp     0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "scotts   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "bbking   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "sherman  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "mpd      0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "\n",
       "               92        93        94        95        96        97        98   \n",
       "         0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \\\n",
       "edu      0.702170  0.368849  0.368849  0.553274  0.661155  0.702170  0.661155   \n",
       "wpi      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "to       0.544925  0.000000  0.000000  0.152003  0.152003  0.152003  0.504943   \n",
       "cs       0.791857  0.000000  0.000000  0.791857  0.395929  0.791857  0.395929   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "msrp     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "scotts   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "bbking   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "sherman  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "mpd      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "               99  \n",
       "         0.000000  \n",
       "edu      0.184425  \n",
       "wpi      0.000000  \n",
       "to       0.152003  \n",
       "cs       0.395929  \n",
       "...           ...  \n",
       "msrp     6.643856  \n",
       "scotts   6.643856  \n",
       "bbking   6.643856  \n",
       "sherman  6.643856  \n",
       "mpd      6.643856  \n",
       "\n",
       "[3654 rows x 100 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.tfidf(base.query('tag==\"doc\"'), 'post').fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div></div> \n",
    "\n",
    "### Tokenização e Lemmatizer\n",
    "\n",
    "<div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     path cantaloupe srv cs cmu edu rochester udel ...\n",
       "1     path cantaloupe srv cs cmu edu da new harvard ...\n",
       "2     newsgroup misc forsale path cantaloupe srv cs ...\n",
       "3     path cantaloupe srv cs cmu edu rochester corne...\n",
       "4     xref cantaloupe srv cs cmu edu path cantaloupe...\n",
       "                            ...                        \n",
       "95    xref cantaloupe srv cs cmu edu newsgroup misc ...\n",
       "96    newsgroup misc forsale subject want lcd overhe...\n",
       "97    newsgroup ingr forsale hsv forsale misc forsal...\n",
       "98    newsgroup misc forsale path cantaloupe srv cs ...\n",
       "99    xref cantaloupe srv cs cmu edu path from scott...\n",
       "Name: post, Length: 200, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base['post'].apply(lambda x: ' '.join([lemmatize_word(word.lower()) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(df, input_col):\n",
    "    vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "\n",
    "a    def lemmatize_word(word):\n",
    "        if len(word) <= 2:\n",
    "            return word\n",
    "        \n",
    "        if word.endswith('ns'):\n",
    "            return word[:-2]\n",
    "        \n",
    "        if word.endswith('s'):\n",
    "            return word[:-1]\n",
    "        \n",
    "        if word.endswith('ing') and len(word) > 5:\n",
    "            if word[-4] == word[-5] and word[-5] not in vowels:\n",
    "                return word[:-4] + word[-3:]\n",
    "            elif word[-3] in vowels:\n",
    "                return word[:-3]\n",
    "            else:\n",
    "                return word[:-2]\n",
    "        \n",
    "        if word.endswith('ly') and len(word) > 4:\n",
    "            return word[:-2]\n",
    "        \n",
    "        if word.endswith('ed') and len(word) > 3:\n",
    "            if word[-3] == word[-4] and word[-4] not in vowels:\n",
    "                return word[:-3] + word[-2:]\n",
    "            else:\n",
    "                return word[:-2]\n",
    "        \n",
    "        return word\n",
    "\n",
    "    df[output_col] = df[input_col].apply(lambda x: ' '.join([lemmatize_word(word.lower()) for word in x.split()]))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')    # faz o download do recurso 'wordnet' do nltk\n",
    "nltk.download('punkt')     # faz o download do recurso 'punkt' do nltk\n",
    "\n",
    "# Cria um objeto 'w_tokenizer' da classe 'WhitespaceTokenizer' do nltk para tokenizar o texto por espaços em branco\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "\n",
    "# Cria um objeto 'lemmatizer' da classe 'WordNetLemmatizer' do nltk para realizar a lematização das palavras\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Define a função 'lemmatizer_text' que recebe um texto como entrada, tokeniza o texto em palavras e lematiza cada palavra\n",
    "def lemmatizer_text(text): \n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "# Cria uma nova coluna 'tokens' na tabela 'base_inicial' que contém uma lista de tokens lematizados para cada texto\n",
    "base_inicial['tokens'] = base_inicial['text'].map(lemmatizer_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div></div> \n",
    "\n",
    "### Remoção de Stopwords\n",
    "\n",
    "<div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Faz o download do recurso 'stopwords' do nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define a lista de stopwords em inglês usando o módulo stopwords do nltk\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# Aplica a função lambda em cada linha da coluna 'text' da tabela 'base_inicial'\n",
    "# A função lambda realiza a tokenização do texto, transforma as palavras em minúsculas e remove as stopwords\n",
    "base_inicial['text'] = base_inicial['text'].apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stopwords))\n",
    "base_inicial['tokens'] = base_inicial['tokens'].apply(lambda words: [word.lower() for word in words if word not in stopwords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cria uma lista de palavras a partir da lista de tokens\n",
    "# w = [j for i in list(itertools.chain(base_inicial['tokens'])) for j in i]\n",
    "\n",
    "# # Instancia um objeto SpellChecker para correção ortográfica\n",
    "# spell = SpellChecker()\n",
    "\n",
    "# if !os.path.isfile(): \n",
    "#     # Cria um dicionário com as palavras únicas da lista, faz a correção ortográfica e associa com a palavra original\n",
    "#     spell_checked = {word: spell.correction(word) for word in pd.Series(w).unique()}\n",
    "\n",
    "#     # Define o caminho do arquivo que irá armazenar o dicionário serializado\n",
    "#     path = '../references/spellcheck.pickle'\n",
    "\n",
    "#     # Abre o arquivo para gravação em modo binário e escreve o objeto serializado\n",
    "#     with open(path, 'wb') as file: \n",
    "#         pickle.dump(spell_checked, file)\n",
    "# else: \n",
    "#     pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export da base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/processed/base_processed.parquet.gzip'\n",
    "\n",
    "if os.path.isfile(path): \n",
    "    answer = input('File already exists, do you want to overwrite? (y/n)')\n",
    "    if answer.lower() in ['s', 'y']:\n",
    "        base_inicial.to_parquet(path, compression='gzip')\n",
    "    else:\n",
    "        raise FileExistsError('File already exists')\n",
    "else: \n",
    "    base_inicial.to_parquet(path, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
