{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperação da informação para conjunto de e-mails\n",
    "\n",
    "Foram utilizadas as bases `20_newsgroups.tar.gz` e `mini_newsgroups.tar.gz` para o projeto:\n",
    "\n",
    "[ICS: E-mail groups](https://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html)\n",
    "\n",
    "<div style=\"padding: 20px; \n",
    "background-color: #de841d; \n",
    "color: white; \n",
    "width: 80vw;\n",
    "border-radius: 8px;\">\n",
    "  <h2>Atenção</h2>\n",
    "  <p>A base de dados possui um total de 22.000 elementos. Destes, 2.000 são utilizados como query, e os outros 20.000 são para consulta. \n",
    "    <br><br> \n",
    "    Após a tokenização, o vocabulário pode chegar a 130.000 palavras, resultando em uma matriz com o termo para cada documento, o que pode ocupar até 30GB de RAM.\n",
    "    <br><br>\n",
    "    Portanto, é importante ter cuidado ao executar o processo, pois é necessário um grande volume de RAM para sua execução.</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa os módulos necessários\n",
    "import os    # Módulo para lidar com funções do sistema operacional\n",
    "import gc    # Módulo para realizar coleta de lixo e gerenciamento de memória\n",
    "import sys\n",
    "\n",
    "import numpy as np   # Módulo para trabalhar com matrizes e funções matemáticas\n",
    "import pandas as pd  # Módulo para trabalhar com dataframes e séries em Python\n",
    "\n",
    "from ir.preprocessing import lemmatize_word  # Importa a função de lematização de palavras\n",
    "from ir.tf_idf import tfidf  # Importa a função de cálculo de TF-IDF\n",
    "\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div></div> \n",
    "\n",
    "## Leitura dos Arquivos\n",
    "\n",
    "Para a pasta da base de dados, os arquivos foram fornecidos em um formato raw, sem indicação de extensão. Cada e-mail é um arquivo dentro de uma pasta que representa um tema.\n",
    "\n",
    "Portanto, nesse caso, é necessário percorrer cada pasta para realizar a leitura e armazenamento da base de dados para análises futuras.\n",
    "\n",
    "<div></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# caminho das queries \n",
    "query_path = '../data/emails/mini_newsgroups/'\n",
    "\n",
    "# caminho dos documentos\n",
    "docs_path = '../data/emails/20_newsgroups/'\n",
    "\n",
    "# Iterate over each file in the directory and its subdirectories\n",
    "def process_files(doc_dir: str): \n",
    "    \n",
    "    database = [] \n",
    "    \n",
    "    for filepath in os.listdir(doc_dir): \n",
    "        \n",
    "        for filename in os.listdir(f'{doc_dir}{filepath}'):\n",
    "\n",
    "            # Open each file individually and read its contents\n",
    "            with open(os.path.join(doc_dir, filepath, filename), 'r') as f:\n",
    "                text_data = f.read().strip()\n",
    "\n",
    "            # Split the header and body of the email\n",
    "            try:\n",
    "                header, body = text_data.split('\\n\\n', maxsplit=1)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # Convert header to a dictionary\n",
    "            # header_dict = {}\n",
    "            # for line in header.split('\\n'):\n",
    "            #     try:\n",
    "            #         # Split the key and value in each header field and store them in a dictionary\n",
    "            #         key, value = line.strip().split(': ', maxsplit=1)\n",
    "            #         header_dict[key] = value\n",
    "            #     except:\n",
    "            #         # If a header field cannot be split properly, skip it and continue\n",
    "            #         continue\n",
    "\n",
    "            # Append the processed data to the list\n",
    "\n",
    "            database.append({'filepath': filepath, \n",
    "                            'filename': filename,\n",
    "                            'body': body, \n",
    "                            # **header_dict,\n",
    "                            # 'text': text_data\n",
    "                            })\n",
    "    return database\n",
    "\n",
    "# tranformation from dict -> dataframe\n",
    "base_doc = pd.DataFrame(process_files(docs_path))\n",
    "\n",
    "base_que = pd.DataFrame(process_files(query_path))\n",
    "\n",
    "# Marcação das bases\n",
    "base_doc['tag'] = 'doc'\n",
    "base_que['tag'] = 'query'\n",
    "\n",
    "# Amostragem para testes\n",
    "base_doc = base_doc.sample(frac=0.5, random_state=42)\n",
    "\n",
    "# junção das bases \n",
    "base = pd.concat([base_doc, base_que])\n",
    "base.reset_index(drop=True, inplace=True)\n",
    "\n",
    "del base_doc, base_que\n",
    "\n",
    "# remove database from memory\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div></div> \n",
    "\n",
    "## Pré-Processamento de Texto\n",
    "\n",
    "Para minimizar possíveis gargalos de processamento e identificação dos termos relevantes, é realizada a remoção de ruídos utilizando regex. Em seguida, é aplicada a tokenização, que consiste na transformação do texto em uma lista de palavras, a fim de possibilitar a aplicação das técnicas de TF-IDF em um modelo vetorial.\n",
    "\n",
    "<div></div> \n",
    "\n",
    "### Remoção de palavras e transformação de minúsculos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\\[a-z]): para encontrar todos os caracteres que começam com uma barra invertida () seguida por uma letra minúscula (a-z);\n",
    "# ([^\\w\\]): para encontrar todos os caracteres que não são letras, números ou barras invertidas ();\n",
    "# (\\S+\\d\\S+): para encontrar todos os trechos de texto que contêm um ou mais caracteres não brancos (\\S), \n",
    "# seguidos por um dígito (\\d), seguidos por mais um ou mais caracteres não brancos (\\S).\n",
    "base['post'] = base['body'].replace(r'(\\\\[a-z])|([^\\w\\\\])|(\\S+\\d\\S+)', ' ', regex=True)\n",
    "\n",
    "# Aplicando as funções str.lower() e str.strip() simultaneamente\n",
    "base['post'] = base['post'].apply(lambda x: x.lower().strip())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div></div> \n",
    "\n",
    "### Tokenização e Lemmatizer\n",
    "\n",
    "**Tokenização:** A tokenização de texto é o processo de dividir um texto em unidades menores, chamadas de tokens. Esses tokens podem ser palavras individuais, caracteres, frases ou até mesmo partes específicas de um texto, dependendo do contexto e das necessidades do processamento de linguagem natural. \n",
    "\n",
    "**Lemmatize:** A lematização de texto é um processo linguístico que visa reduzir as palavras em sua forma base ou forma lematizada. O objetivo é transformar palavras flexionadas em sua forma canônica, chamada de \"lema\" ou \"base\". Por exemplo, a lematização transforma palavras como \"correndo\" em \"correr\", \"carros\" em \"carro\" e assim por diante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        todd steve write chuck petch write now it appe...\n",
       "1        boston globe wednesday april 21 col 4 bodie fo...\n",
       "2        nl chicago wait til next year new york bunch o...\n",
       "3        i recent had a case of shingle and my doctor w...\n",
       "4        for sale 2 amiga commodore amiga best offer ra...\n",
       "                               ...                        \n",
       "11972    in article huston acces digex com herb huston ...\n",
       "11973    i just start read thi newsgroup and haven t be...\n",
       "11974    blesed are those who hunger and thirst for rig...\n",
       "11975    i m curiou to know if christia ever read book ...\n",
       "11976    in article prl csi dit csiro au peter lamb wri...\n",
       "Name: post, Length: 11977, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base['post'] = base['post'].apply(lambda x: ' '.join([lemmatize_word(word.lower()) for word in x.split()]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificação das query / docs\n",
    "\n",
    "Foi feita uma separação do index das query, para pode fazer uma localização do na base origina após o TF-IDF, dado que o TF-IDF reseta os index dos termos por documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_index = base.query('tag==\"doc\"').index\n",
    "q_index = base.query('tag==\"query\"').index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento dos dados\n",
    "\n",
    "Aplicação das técnicas estatísticas no conjunto de palavras por documento\n",
    "\n",
    "### TF IDF\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) é uma medida estatística usada para avaliar a importância de um termo em um documento em relação a uma coleção de documentos. É amplamente utilizado em processamento de linguagem natural e recuperação de informações.\n",
    "\n",
    "O TF-IDF é calculado levando em consideração dois fatores principais:\n",
    "\n",
    "Frequência do termo (TF - Term Frequency): Mede a frequência com que um termo específico aparece em um documento. Quanto mais vezes um termo aparece, maior é sua relevância no documento.\n",
    "\n",
    "Frequência inversa do documento (IDF - Inverse Document Frequency): Mede a raridade de um termo em relação a uma coleção de documentos. Quanto menos frequente um termo é em outros documentos da coleção, maior é o seu valor IDF e maior será seu peso para distinguir a importância desse termo no documento atual.\n",
    "\n",
    "O TF-IDF é calculado multiplicando-se o TF pelo IDF para cada termo em um documento. Dessa forma, termos frequentes no documento e raros na coleção terão um valor TF-IDF mais alto, indicando sua relevância para o documento em questão.\n",
    "\n",
    "Essa medida é amplamente utilizada em tarefas como recuperação de informações, classificação de texto, sumarização automática e agrupamento de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tfidf(base, 'post').T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranqueamento\n",
    "\n",
    "O ranqueamento de documentos utilizando o TF-IDF (Term Frequency-Inverse Document Frequency) é um método utilizado para ordenar documentos em uma coleção com base na relevância em relação a uma consulta de busca.\n",
    "\n",
    "Nesse método, cada documento é representado por um vetor numérico, no qual cada dimensão corresponde a um termo presente na coleção de documentos. O valor de cada dimensão é calculado utilizando a fórmula do TF-IDF, que leva em consideração a frequência do termo no documento e a raridade do termo na coleção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_geral = linear_kernel(weights.iloc[d_index], weights.iloc[q_index])\n",
    "rank_geral = pd.DataFrame(rank_geral, index=d_index, columns=q_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 3093/3093 - Doc: 1730/3092"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         3 function calls in 0.000 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.000    0.000 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}"
     ]
    }
   ],
   "source": [
    "def calcular_resultados_relevantes(q_index: list, base: pd.DataFrame) -> 'resultados_relevantes[dict], resultados_sistema[dict]':\n",
    "    resultados_sistema = {}\n",
    "\n",
    "    for q in q_index: \n",
    "        resultados_sistema[q] = rank_geral[q].sort_values(ascending=False).index\n",
    "\n",
    "    resultados_relevantes = {}\n",
    "\n",
    "    for q in q_index:\n",
    "        q_genre = base.iloc[q]['genres']\n",
    "\n",
    "        k = []\n",
    "\n",
    "        for d in resultados_sistema[q]:\n",
    "            d_genre = base.iloc[d]['genres']\n",
    "            \n",
    "            # Verifica qual lista de gêneros é menor para otimizar a comparação\n",
    "            if len(d_genre) > len(q_genre):\n",
    "                comparativo_menor = q_genre\n",
    "                comparativo_maior = d_genre\n",
    "            else:\n",
    "                comparativo_menor = d_genre\n",
    "                comparativo_maior = q_genre\n",
    "            \n",
    "            # Verifica se há pelo menos um gênero em comum entre as listas\n",
    "            partial_relevance = any(i in comparativo_maior for i in comparativo_menor)\n",
    "            \n",
    "            if partial_relevance:\n",
    "                k.append(d)\n",
    "        \n",
    "        print(f'\\rQuery: {q}/{q_index.max()} - Doc: {d}/{d_index.max()}', end='')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        resultados_relevantes[q] = k\n",
    "        \n",
    "    return resultados_relevantes, resultados_sistema\n",
    "\n",
    "resultados_relevantes, resultados_sistema = calcular_resultados_relevantes(q_index, base)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas\n",
    "\n",
    "P@N: essa métrica mede a proporção de documentos relevantes presentes entre os 10 primeiros resultados retornados por um sistema de busca em resposta a uma consulta.\n",
    "\n",
    "MAP (Mean Average Precision): o MAP leva em consideração a precisão e a ordenação dos resultados retornados por um sistema de busca em relação a um conjunto de consultas. Ele mede a média das precisões médias de cada consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_p_n_media(resultados_relevantes, resultados_sistema, n):\n",
    "    \"\"\"\n",
    "    Calcula a média da precisão P@n para um conjunto de consultas e seus resultados relevantes.\n",
    "\n",
    "    Parâmetros:\n",
    "    - resultados_relevantes (dict): Um dicionário que mapeia cada consulta aos seus resultados relevantes.\n",
    "    - resultados_sistema (dict): Um dicionário que mapeia cada consulta aos resultados retornados pelo sistema.\n",
    "    - n (int): O número de resultados a considerar para o cálculo da precisão.\n",
    "\n",
    "    Retorno:\n",
    "    - p_n_media (float): A média da precisão P@n para todas as consultas.\n",
    "\n",
    "    \"\"\"\n",
    "    def calcular_p_n(resultados, relevantes):\n",
    "        \"\"\"\n",
    "        Calcula a precisão P@n para uma lista de resultados e seus resultados relevantes.\n",
    "\n",
    "        Parâmetros:\n",
    "        - resultados (list): Uma lista de resultados retornados pelo sistema.\n",
    "        - relevantes (list): Uma lista de resultados relevantes para a consulta.\n",
    "\n",
    "        Retorno:\n",
    "        - p_n (float): A precisão P@n.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(resultados) > n:\n",
    "            resultados = resultados[:n]  # Considerar apenas os primeiros n resultados\n",
    "        num_relevantes = len(set(resultados) & set(relevantes))  # Contar quantos resultados relevantes foram encontrados\n",
    "        p_n = num_relevantes / n  # Calcular a precisão P@n\n",
    "        return p_n\n",
    "\n",
    "    p_n_total = 0\n",
    "    for consulta, relevantes in resultados_relevantes.items():\n",
    "        resultados = resultados_sistema.get(consulta, [])  # Obtém os resultados retornados pelo sistema para a consulta\n",
    "        p_n = calcular_p_n(resultados, relevantes)\n",
    "        p_n_total += p_n\n",
    "\n",
    "    p_n_media = p_n_total / len(resultados_relevantes)\n",
    "    return p_n_media\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média do P@10: 0.719268030139934\n",
      "Média do P@20: 0.6967168998923577\n",
      "Média do P@50: 0.6745748116254034\n",
      "Média do P@100: 0.658902045209902\n"
     ]
    }
   ],
   "source": [
    "for x in [10, 20, 50, 100]: \n",
    "    print(f\"Média do P@{x}: {calcular_p_n_media(resultados_relevantes, resultados_sistema, n=x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5882758350175267"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def average_precision(relevantes, recomendados):\n",
    "    \"\"\"\n",
    "    Calcula a Média de Precisão (Average Precision) para um conjunto de itens relevantes e itens recomendados.\n",
    "\n",
    "    Parâmetros:\n",
    "    - relevantes (list): Uma lista contendo os itens relevantes.\n",
    "    - recomendados (list): Uma lista contendo os itens recomendados.\n",
    "\n",
    "    Retorno:\n",
    "    - ap (float): O valor da Média de Precisão.\n",
    "\n",
    "    \"\"\"\n",
    "    relevancia_cumulativa = 0\n",
    "    precision_cumulativa = 0\n",
    "    num_relevantes = len(relevantes)\n",
    "    ap = 0\n",
    "\n",
    "    for i, rec in enumerate(recomendados):\n",
    "        if rec in relevantes:\n",
    "            relevancia_cumulativa += 1\n",
    "            precision_cumulativa += relevancia_cumulativa / (i + 1)\n",
    "\n",
    "    if num_relevantes > 0:\n",
    "        ap = precision_cumulativa / num_relevantes\n",
    "\n",
    "    return ap\n",
    "\n",
    "\n",
    "def mean_average_precision(resultados_relevantes, resultados_sistema):\n",
    "    \"\"\"\n",
    "    Calcula a Média de Precisão (MAP) para um conjunto de consultas, seus resultados relevantes e resultados retornados pelo sistema.\n",
    "\n",
    "    Parâmetros:\n",
    "    - resultados_relevantes (dict): Um dicionário que mapeia cada consulta aos seus resultados relevantes.\n",
    "    - resultados_sistema (dict): Um dicionário que mapeia cada consulta aos resultados retornados pelo sistema.\n",
    "\n",
    "    Retorno:\n",
    "    - map (float): O valor da Média de Precisão Média (MAP) para todas as consultas.\n",
    "\n",
    "    \"\"\"\n",
    "    map = 0\n",
    "    num_consultas = len(resultados_relevantes)\n",
    "\n",
    "    for q in resultados_relevantes:\n",
    "        relevantes = resultados_relevantes[q]\n",
    "        recomendados = resultados_sistema[q]\n",
    "        ap = average_precision(relevantes, recomendados)\n",
    "        map += ap\n",
    "\n",
    "    if num_consultas > 0:\n",
    "        map /= num_consultas\n",
    "\n",
    "    return map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aplicar o MAP nas consultas\n",
    "mean_average_precision(resultados_relevantes, resultados_sistema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
